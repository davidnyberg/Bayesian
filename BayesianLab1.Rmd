---
title: "TDDE07-lab1"
author: "David Nyberg (davny376)"
date: "4/2/2020"
output: pdf_document
---

## 1) Bernoulli... again

a) Drawing random numbers from our posterior $\theta$|y ~ Beta($\alpha$ + s, $\beta$ + f ) with 5 succeses and 15 failures we can see graphically how the posterior converges to the true values when we increase the number over draws from only 100 up to 100,000 gets us a smooth curve.  

```{r echo=FALSE}
poster100 <- rbeta(100, 2+5, 2+15)
poster1000 <- rbeta(1000, 2+5, 2+15)
poster100000 <- rbeta(100000, 2+5, 2+15)
hist(poster100, 50, main = "100 Draws", xlab="")
```

```{r echo=FALSE}
hist(poster1000, 50, main = "1000 Draws", xlab="")
```

```{r echo=FALSE}
hist(poster100000, 50, main = "100000 Draws", xlab="")

a <- 2 + 5
b <- 2 + 15
standard_deviation <- sqrt((a*b) / ((a+b)^2 * (a+b+1) )) 
true_mean <- a / (a + b)
```

Calculating the true standard deviation and the true mean we can see the results from 100000 draws looks very accurate in comparison to the true values. In the plot we can see a convergence of the values as the number of samples increases, these values are approaching the true values printed below. 

```{r echo = FALSE}
means <- c()
std_dev <- c()
for (i in 1:1000){
  posteriordraws <- rbeta(i, a, b) 
  means[i] <- mean(posteriordraws)
  std_dev[i] <- sd(posteriordraws)
}
plot(1:1000, means, type = 'l', ylim = c(0.05,0.4), main = "Convergence of sample mean and std deviation", ylab = "Std_dev(red), mean(blue)", col = "blue", xlab = "# samples")
lines(std_dev, col = "red")

print(paste0("True standard deviation: ", standard_deviation))
print(paste0("True mean: ", true_mean))
```

b) Use simulation (nDraws = 10000) to compute the posterior probability Pr($\theta$ > 0.3|y) compared with the simulated we can see the value is almost the same in both results. This shows that our posterior propability is very close to the true probability.

```{r echo=FALSE}
true_prob <- pbeta(0.3, 2+5, 2+15, lower.tail = FALSE)
print(paste0("The true probability theta > 0.3: ", true_prob))
poster <- rbeta(10000, 2+5, 2+15) 
sumnewsamples <- sum(poster > 0.3) / 10000
print(paste0("The sampled probability theta > 0.3: ", sumnewsamples))
```

c) Here is a plot of the posterior distribution of the log odds by simulation showing density of the log odds.

```{r echo=FALSE}
phi <- log( poster / (1 - poster))
#density(phi)
hist(phi, freq = FALSE)
lines(density(phi))
```

## 2) Log-normal distribution and the Gini coefficient.

a) Start by simulating 10,000 draws from the posterior of $\sigma$ ^2 with $\mu$ of 3.7 is assumed. Comparing this simulated result with the theoretical inv chisquared we can see that they are similar. In the plot the histogram is the simulated inv chi squared with the calculated tau squared. The line is the true theoretical distribution. 

```{r echo=FALSE}
#2
data <- c(44,25, 45, 52, 30, 63, 19, 50, 34, 67)
u <- 3.7
n <- length(data)
t <- sum((log(data) - u) ^ 2) / n
#simulating variance from an inverse chi squared distribtuition
sigmasquared <- (n*t)/rchisq(10000, n)

#pdf for scaled inv chi square in wikipedia, seems different than course page pdf , seq is range of x's we want(0-1 from sigmasquareds), degrees of freedom = n 
theoretical <- (((t * n/2)^(n/2)) / (gamma(n/2)) )*( exp((-n*t) / (seq(0,1, .001) * 2)) / seq(0 , 1, .001) ^ (1 + n/2) )
hist(sigmasquared, 100, xlim=c(0,1), freq = FALSE)
lines(seq(.001,1,.001), theoretical[2:1001])
```

b) Using the gini coefficient calculation with the previous results as input the following plot shows the density of the gini coefficient.

```{r echo=FALSE}
#b
G <- 2 * pnorm(sqrt(sigmasquared)/sqrt(2)) - 1
hist(G, 100, freq = FALSE)
lines(density(G))
```

c) 
The 90% equal tail credible interval for G: (0.158,0.346) at 5% and 95% respectively. 
Kernel density estimate of 90% highest posterior density: (0.159, 0.337) at 5% and 95% respectively. 
Plotted below is the comparison of the two interval. The two intervals are almost identical. 

```{r echo=FALSE}
#c
equal_tail <- quantile(G, c(.05,.95))

#kernel density estimate
gdens <- density(G)
#plot(gdens)

#source: https://stats.stackexchange.com/questions/240749/how-to-find-95-credible-interval

#sort_gy <- sort(gdens$y, decreasing = TRUE)
test <- cumsum(gdens$y) / sum(gdens$y)
low <- gdens$x[which(test >= .05)][1]
#sort_gx <- sort(gdens$x, decreasing = FALSE)
hi <- gdens$x[which(test >= .95)][1]

#sort_g <- normalize(gdens$y)

#total_sum <- sum(sort_gy)
#sum of a = 1
#a <- sort_gy / total_sum
#crit <- a[which(cumsum(a) >= 0.95)[1]] * total_sum
  
#choose 90% of data from the density
#a <- quantile(test, c(.05, .95))
#b <- sort_g[sort_g < quantile(sort_g, .95) & sort_g > quantile(sort_g,.05) ]
#options(scipen=999)
#gdens$x[max(b)]
#gdens$x[which.max(b)]

plot(density(G))
abline(v=equal_tail[1], col = 'red')
abline(v=equal_tail[2], col = 'red')
abline(v=low)
abline(v=hi)
```

## 3) von Mises distribution

a) The plot below shows the posterior distribution of K over a sequence of k values. 
b) From this plot we can see the mode from this data is right above two. Checking this by seeing which k values correspond the the max of the posterior we get a result of 2.12 being the mode. 

```{r echo=FALSE}
#3
#data <- c(40, 303, 326, 285, 296, 314, 20, 308, 299, 296)
radians <- c(-2.44, 2.14, 2.54, 1.83, 2.02, 2.33, -2.79, 2.23, 2.07, 2.02)

posteriorfunc <- function(kvals, data = radians){
  u <- 2.39
  Io <- besselI(kvals, 0)
  #likelihood x prior 
  return( prod( (exp(kvals * cos(data - u))) / (2 * pi * Io) )  * dexp(kvals)  )
}

options(scipen=999)
k_values <- seq(0,10,0.01)
posterior <- c()
for(x in 1:length(k_values)){
  posterior[x] <- posteriorfunc(k_values[x]) #function defaults to radians data if not supplied
}

plot(x= k_values/10, y = posterior/sum(posterior))
#this plots from 0-1 and the area under curve is now normalized to 1
#sum(posterior/sum(posterior))
#k_values[which.max(posterior)]
```

```{r ref.label=knitr::all_labels(), echo = T, eval=F}

```




