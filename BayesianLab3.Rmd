---
title: "BayesianLab3"
author: "David Nyberg"
date: "5/3/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1
```{r}
rainfall <- read.table("rainfall.dat.txt")
rainfall <- rainfall$V1
```

```{r}
#priors
u_0 <- 30
t_sq_0 <- 30
sigma_sq_0 <- 30
v_0 <- 5

n <- length(rainfall)
#used in first iteration of for loop
w <- (n/sigma_sq_0) / (n/sigma_sq_0) + (1/t_sq_0)
u_n <- w * mean(rainfall) + (1-w)*u_0

inv_t_sq_n <- n/sigma_sq_0 + 1/t_sq_0
t_sq_n <- 1/inv_t_sq_n
v_n <- n + u_0
n_draws <- 2000 # Number of draws

#sigma_sample <- (n*t_sq_0)/rchisq(10000, n)

gibbsDraws <- matrix(0,n_draws,2)
#implement gibbs sampler
#code from mattias example, edited draws from lecture7 posteriors, equations from lecture2
for (i in 1:n_draws){
  w <- (n/sigma_2_draw) / (n/sigma_2_draw) + (1/t_sq_0)
  u_n <- w * mean(rainfall) + (1-w)*u_0
  inv_t_sq_n <- n/sigma_2_draw + 1/t_sq_0
  t_sq_n <- 1/inv_t_sq_n

  u_draw <- rnorm(1, mean = u_n, sd = sqrt(t_sq_n))
  gibbsDraws[i,1] <- u_draw
  
  sigma_2_draw <- (v_n * (v_0 * sigma_sq_0 + sum((rainfall - u_draw)^2)) /(n + v_0) ) / rchisq(1, n)
  gibbsDraws[i,2] <- sigma_2_draw
  
}

#from mattias
a <- cumsum(gibbsDraws[,1])/seq(1,n_draws)
plot(1:n_draws, a, type='l')
```
```{r}
# Estimating a simple mixture of normals
# Author: Mattias Villani, IDA, Linkoping University. http://mattiasvillani.com

##########    BEGIN USER INPUT #################
# Data options
x <- as.matrix(rainfall)

# Model options
nComp <- 2    # Number of mixture components

# Prior options
alpha <- 10*rep(1,nComp) # Dirichlet(alpha)
muPrior <- rep(0,nComp) # Prior mean of mu
tau2Prior <- rep(10,nComp) # Prior std of mu
sigma2_0 <- rep(var(x),nComp) # s20 (best guess of sigma2)
nu0 <- rep(4,nComp) # degrees of freedom for prior on sigma2

# MCMC options
nIter <- 100 # Number of Gibbs sampling draws

# Plotting options
plotFit <- TRUE
lineColors <- c("blue", "green", "magenta", 'yellow')
sleepTime <- 0.01 # Adding sleep time between iterations for plotting
################   END USER INPUT ###############

###### Defining a function that simulates from the 
rScaledInvChi2 <- function(n, df, scale){
  return((df*scale)/rchisq(n,df=df))
}

####### Defining a function that simulates from a Dirichlet distribution
rDirichlet <- function(param){
  nCat <- length(param)
  piDraws <- matrix(NA,nCat,1)
  for (j in 1:nCat){
    piDraws[j] <- rgamma(1,param[j],1)
  }
  piDraws = piDraws/sum(piDraws) # Diving every column of piDraws by the sum of the elements in that column.
  return(piDraws)
}

# Simple function that converts between two different representations of the mixture allocation
S2alloc <- function(S){
  n <- dim(S)[1]
  alloc <- rep(0,n)
  for (i in 1:n){
    alloc[i] <- which(S[i,] == 1)
  }
  return(alloc)
}

# Initial value for the MCMC
nObs <- length(x)
S <- t(rmultinom(nObs, size = 1 , prob = rep(1/nComp,nComp))) # nObs-by-nComp matrix with component allocations.
mu <- quantile(x, probs = seq(0,1,length = nComp))
sigma2 <- rep(var(x),nComp)
probObsInComp <- rep(NA, nComp)

# Setting up the plot
xGrid <- seq(min(x)-1*apply(x,2,sd),max(x)+1*apply(x,2,sd),length = 100)
xGridMin <- min(xGrid)
xGridMax <- max(xGrid)
mixDensMean <- rep(0,length(xGrid))
effIterCount <- 0
ylim <- c(0,2*max(hist(x)$density))


for (k in 1:nIter){
  #message(paste('Iteration number:',k))
  alloc <- S2alloc(S) # Just a function that converts between different representations of the group allocations
  nAlloc <- colSums(S)
  #print(nAlloc)
  # Update components probabilities
  pi <- rDirichlet(alpha + nAlloc)
  
  # Update mu's
  for (j in 1:nComp){
    precPrior <- 1/tau2Prior[j]
    precData <- nAlloc[j]/sigma2[j]
    precPost <- precPrior + precData
    wPrior <- precPrior/precPost
    muPost <- wPrior*muPrior + (1-wPrior)*mean(x[alloc == j])
    tau2Post <- 1/precPost
    mu[j] <- rnorm(1, mean = muPost, sd = sqrt(tau2Post))
  }
  
  # Update sigma2's
  for (j in 1:nComp){
    sigma2[j] <- rScaledInvChi2(1, df = nu0[j] + nAlloc[j], scale = (nu0[j]*sigma2_0[j] + sum((x[alloc == j] - mu[j])^2))/(nu0[j] + nAlloc[j]))
  }
  
  # Update allocation
  for (i in 1:nObs){
    for (j in 1:nComp){
      probObsInComp[j] <- pi[j]*dnorm(x[i], mean = mu[j], sd = sqrt(sigma2[j]))
    }
    S[i,] <- t(rmultinom(1, size = 1 , prob = probObsInComp/sum(probObsInComp)))
  }
  
  # Printing the fitted density against data histogram
  if (plotFit && (k%%1 ==0100){
    effIterCount <- effIterCount + 1
    #hist(x, breaks = 20, freq = FALSE, xlim = c(xGridMin,xGridMax), main = paste("Iteration number",k), ylim = ylim)
    mixDens <- rep(0,length(xGrid))
    components <- c()
    for (j in 1:nComp){
      compDens <- dnorm(xGrid,mu[j],sd = sqrt(sigma2[j]))
      mixDens <- mixDens + pi[j]*compDens
      lines(xGrid, compDens, type = "l", lwd = 2, col = lineColors[j])
      components[j] <- paste("Component ",j)
    }
    mixDensMean <- ((effIterCount-1)*mixDensMean + mixDens)/effIterCount
    
    lines(xGrid, mixDens, type = "l", lty = 2, lwd = 3, col = 'red')
    legend("topleft", box.lty = 1, legend = c("Data histogram",components, 'Mixture'), 
           col = c("black",lineColors[1:nComp], 'red'), lwd = 2)
    Sys.sleep(sleepTime)
  }
  
}

hist(x, breaks = 20, freq = FALSE, xlim = c(xGridMin,xGridMax), main = "Final fitted density")
lines(xGrid, mixDensMean, type = "l", lwd = 2, lty = 4, col = "red")
lines(xGrid, dnorm(xGrid, mean = mean(x), sd = apply(x,2,sd)), type = "l", lwd = 2, col = "blue")
legend("topright", box.lty = 1, legend = c("Data histogram","Mixture density","Normal density"), col=c("black","red","blue"), lwd = 2)

#########################    Helper functions    ##############################################
```


```{r}
#c
plot(density(rainfall))
lines(xGrid,mixDensMean, type = "l", col = "red")
lines(xGrid,dnorm(xGrid,mean(gibbsDraws[,1]), apply(x,2,sd)), col = 'blue')
```
## Assignemnt 2. Metropolis Random Walk for Poisson regression.
```{r}
#2a
ebay <- read.table("eBayNumberOfBidderData.dat.txt", header = TRUE)
summary(ebay)

glm_model <- glm(nBids ~ . -Const, family= "poisson", data = ebay)
summary(glm_model)
```
```{r}
#2b
library(mvtnorm)
X <- as.matrix(ebay[,-c(1)])
mean_vector <- c(0,0,0,0,0,0,0,0,0)
sigma <- 100 * solve(t(X) %*% X)
y <- ebay$nBids
a <- rmvnorm(100, mean_vector, 100 * solve(t(X) %*% X))
hist(a,100)

#lecture 6
LogPostPoisson <- function(betaVect,y,X,mu,Sigma){
  nPara <- length(betaVect)
  linPred <- betaVect %*% t(X)
                                      
  logLik <- sum(linPred*y - exp(linPred))
  if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
  logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE)
  #returning posterior
  return(logLik + logPrior)
}

initVal <- as.vector(rep(0,dim(X)[2]))
OptimResults <- optim(initVal,LogPostPoisson,gr=NULL,y,X,mean_vector,sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)

OptimResults$par
OptimResults$hessian
glm_model$coefficients
```
```{r}
j_inv <- -solve(OptimResults$hessian)
betas <- OptimResults$par
draws <- rmvnorm(10000, betas, j_inv)
hist(draws[,1], 100)
```

```{r}
#E = our inverse hessian - j_inv
#c is tuning parameter 
rwm_sampler <- function(theta, iterations, Eps, c, LogPostFunc,...) {
  thetaVec = c()
  for (i in 1:iterations){
    #sample proposal
    theta_p <- rmvnorm(1, theta, c*Eps)
    
    #Metropolis acceptance probability , from 'how to code rwm'
    #this means the posterior function used must return Logs as this is a exponenent trick
    alpha = min(1, exp(LogPostFunc(theta,...) - LogPostFunc(theta_p,...)))
    random = runif(1)
    
    #with probability alpha, set theta(i) = theta_p
    print(alpha)
    if(alpha < random){
      #when we start to converge this will almost always be false meaning we keep taking the same previous values
      #instead of changing the theta vals, resulting in convergence 
      theta = theta_p
    }
    thetaVec = rbind(thetaVec, theta)
  }
  return(thetaVec)
}

res = rwm_sampler(mean_vector, 2000, j_inv, 0.1, LogPostPoisson, y, X, mean_vector, sigma)
for (x in 1:9){
  plot(res[,x], type = 'l')
}

x <- c(1,1,1,1,0,0,0,1,.5)

betas <- res[1000,]
a <- rpois(5000, exp(t(x) * betas))

barplot(table(a))
```


